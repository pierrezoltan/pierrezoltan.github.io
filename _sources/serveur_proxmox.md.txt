# **Serveur Proxmox**

## Objectif et environnement

### Objectif

Installation et configuration d’un cluster Proxmox avec trois serveurs en haute disponibilité sur serveurs. Tolérance de panne d’un serveur.

### Environnement

Matériels :

- 3 x Dell PowerEdge r240 avec Proxmox 7.1-7

Adressage des VMS :

- Réseau interne : 172.16.0.0/16

- IP internes : 172.28.1.250, 172.28.1.250, 172.28.1.250

- Réseau externe : 192.168.10.xx/26 (en DHCP fournie par le PEI)

### Documentation

<https://www.youtube.com/watch?app=desktop&v=Y-n73ciSsZw>

### Configuration préalable

```{warning} **Nous allons utiliser des disques formattés en Ceph pour la haut disponibilité. Ces disques doivent être non RAID ou RAID0 à 1 disque, sinon Ceph ne pourra pas s’installer.**
```

## Processus

### Création d’un Cluster

Nous avons créé un cluster sur le 1<sup>er</sup>
serveur (172.28.1.250/16) :

<img src="./media/image5.png"
style="width:6.2092in;height:1.3231in" />

Nommé le cluster puis create :

<img src="./media/image6.png"
style="width:5.24031in;height:1.77108in" />

Le cluster a été correctement créé :

<img src="./media/image7.png"
style="width:6.27014in;height:1.73819in" />

Une fois le cluster créé, nous avons copié join information (clé
d’agrégation)

<img src="./media/image8.png"
style="width:6.27014in;height:1.96806in" />

Sur le 2<sup>e</sup> serveur (pve2 : 172.28.1.251/16)

Menu Cluster \> Joindre le cluster

<img src="./media/image9.png"
style="width:5.58148in;height:2.09313in" />

Le serveur est bien arrivé sur le cluster. Nous avons fait la même
manipulation pour le 3<sup>e</sup> serveur (172.28.1.252/16).

<img src="./media/image10.png"
style="width:6.27014in;height:1.23819in" />

#### Installation Ceph

Installation Ceph sur les serveurs

Menu \> Ceph

<img src="./media/image11.png"
style="width:1.88568in;height:0.89596in" />

<img src="./media/image12.png"
style="width:3.22762in;height:2.28085in" />

<img src="./media/image13.png"
style="width:3.97535in;height:2.80066in" />

Nous avons installé Ceph sur les deux autres serveurs également.

<img src="./media/image14.png"
style="width:3.95585in;height:2.79081in" />

<img src="./media/image15.png"
style="width:3.97031in;height:2.80635in" />

#### Ajout des Monitors et Managers

Cette fonctionnalité donne des droits pour afficher et manager les
différents serveurs Proxmox.

Menu \> Ceph \> Monitor

<img src="./media/image16.png"
style="width:6.27014in;height:0.95278in" />

<img src="./media/image17.png"
style="width:2.60453in;height:1.11474in" />

<img src="./media/image18.png"
style="width:5.71955in;height:1.56272in" />

<img src="./media/image19.png"
style="width:2.59411in;height:1.11474in" />

<img src="./media/image20.png"
style="width:5.56328in;height:1.57314in" />

#### Création d’un pool de stockage

Menu Ceph \> OSD \> Create OSD

<img src="./media/image21.png"
style="width:5.25073in;height:1.76066in" />

<img src="./media/image22.png"
style="width:6.27014in;height:0.97778in" />

Nous avons effectué la même manipulation sur les deux autres serveurs.

<img src="./media/image23.png"
style="width:6.27014in;height:1.49722in" />

#### Création d’un pool

Menu \> Ceph \> Pools

<img src="./media/image24.png"
style="width:5.24031in;height:2.50035in" />

<img src="./media/image25.png"
style="width:5.82092in;height:2.34861in" />

Ceph stockage a été créé.

#### Mise en place de la haute disponibilité (HA)

Menu \> HA \> Groups

<img src="./media/image26.png"
style="width:3.28802in;height:2.70624in" />

<img src="./media/image27.png"
style="width:3.23946in;height:2.64051in" />

<img src="./media/image28.png"
style="width:3.43048in;height:2.80738in" />

#### Test de la migration d’une VM

Nous avons créé un conteneur test sur le pve1

<img src="./media/image29.png"
style="width:5.24031in;height:1.29185in" />

Nous avons ajouté le conteneur dans la haute disponibilité pve1-ha

<img src="./media/image30.png"
style="width:3.79412in;height:1.60029in" />

<img src="./media/image31.png"
style="width:6.27014in;height:1.82083in" />

Nous avons arrêté le pve1.

<img src="./media/image32.png"
style="width:2.26268in;height:2.59265in" />

Le conteneur est bien arrivé sur le pv3. Par la suite nous avons rallumé
pve1.

<img src="./media/image33.png"
style="width:2.47951in;height:2.20864in" />

Le conteneur est retourné automatiquement sur pve1(3 min après avoir
démarré pv1).

#### Test de la haute disponibilité lors d’une coupure d’un serveur

Nous avons lancé une copie d’un fichier volumineux à partir de la
partition D:\\ du l’AD sur le pve1 vers un dossier partage se trouvant
sur le serveur SAMBA sur le pve3.

<img src="./media/image34.png"
style="width:3.76525in;height:1.29167in" /><img src="./media/image35.png"
style="width:1.97291in;height:3.69502in" />

<img src="./media/image36.png"
style="width:5.50339in;height:2.41433in" />

En paralelle, nous avons lancé un ping continue depuis CLI-DEB-01 sur
pve3 vers [www.google.com](http://www.google.com). Le client accède à
l’internet via FW-PROXY se trouvant sur le pve1.

<img src="./media/image37.png"
style="width:4.29037in;height:1.52445in" />

**NB** : chaque VM possède un snapshot avec sa dernière bonne
configuration. Lors d’une migration, la VM bascule avec son snapshot !

Configuration des groupes HA

<img src="./media/image38.png"
style="width:5.20906in;height:1.22934in" />

Ajout des ressources pour effectuer le test

<img src="./media/image39.png"
style="width:6.27014in;height:1.54375in" />

<img src="./media/image40.png"
style="width:6.27014in;height:1.40903in" />

Etape1

<img src="./media/image41.png"
style="width:3.97972in;height:1.46895in" />

<img src="./media/image42.png"
style="width:4.81317in;height:0.64592in" />

Les VM sont arrivées sur le pve3

<img src="./media/image43.png"
style="width:2.52119in;height:3.02126in" />

<img src="./media/image44.png"
style="width:4.82359in;height:0.90638in" />

<img src="./media/image45.png"
style="width:6.27014in;height:3.70486in" />

Nous avons rejoué la séquence en rebranchent le pve1

<img src="./media/image46.png"
style="width:6.27014in;height:1.89514in" />

**Etape2**

Les VM AD et PROXY sont en état **migrate**

<img src="./media/image47.png"
style="width:6.27014in;height:1.94028in" />

Pendant la copie la migration du l’AD n’a pas été effective.

**Etape 3**

Pf a basculé complètement sans aucune coupure

<img src="./media/image48.png"
style="width:6.27014in;height:1.61319in" />

<img src="./media/image49.png"
style="width:6.27014in;height:1.63472in" />

**Etape 4**

Fin de la copie du fichier depuis l’AD vers le partage SAMBA.

<img src="./media/image50.png"
style="width:6.27014in;height:1.14444in" />

Pendant la migration l’AD continue fournir des services, notamment la
résolution DNS. L’AD est toujours 100% fonctionnel.

<img src="./media/image51.png"
style="width:5.53202in;height:0.76052in" />

**Etape 5**

La migration du l’AD est effective

<img src="./media/image52.png"
style="width:2.30416in;height:3.04821in" />

### Suppression d’un cluster

```bash
systemctl stop corosync pve-cluster  
pmxcfs -l  
rm -rf /etc/corosync/\*  
rm -rf /etc/pve/corosync.conf  
killall pmxcfs  
systemctl start pve-cluster
```

## Conclusion

La mise en place d’un cluster est terminée. Les images concernées par la
haute disponibilité doivent être crées sur la partition répliquée prévue
à cette effet(ceph-stockage).

Sur Proxmox il y a une différence notable entre migration voulu ou
depuis et vers un serveur actif et migration automatique suite d’une
panne. Dans le premier cas (reprise d’activités), nous n’avons pas
constaté de coupure de service. La migration est plus lente. Lors de la
simulation d’une panne la migration est automatique avec des coupures de
service, néanmoins la migration est plus rapide.

```{eval-rst}
.. raw:: html
   :file: flyout.html
```
